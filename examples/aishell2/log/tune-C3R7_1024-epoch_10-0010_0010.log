-----------  Configuration Arguments -----------
alpha_from: 0.0
alpha_to: 10.0
batch_size: 256
beam_size: 500
beta_from: 0.0
beta_to: 10.0
cutoff_prob: 1.0
cutoff_top_n: 40
error_rate_type: cer
lang_model_path: models/lm/zhidao_giga.klm
mean_std_path: data/aishell2/mean_std.npz
model_path: checkpoints/aishell2-C3R71024/step_final
num_alphas: 5
num_batches: -1
num_betas: 5
num_conv_layers: 3
num_proc_bsearch: 12
num_rnn_layers: 7
rnn_layer_size: 1024
share_rnn_weights: 0
specgram_type: linear
trainer_count: 8
tune_manifest: data/Wang/16k/manifest.dev
use_gpu: 1
use_gru: 1
vocab_path: data/aishell2/vocab.txt
------------------------------------------------
2020-03-12 13:33:35,980-INFO: begin to initialize the external scorer for decoding
2020-03-12 13:33:37,865-INFO: language model: is_character_based = 1, max_order = 5, dict_size = 0
2020-03-12 13:33:37,866-INFO: end initializing scorer
2020-03-12 13:33:37,866-INFO: start tuning ...
W0312 13:33:48.661011  3913 device_context.cc:236] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 10.2, Runtime API Version: 10.0
W0312 13:33:48.664369  3913 device_context.cc:244] device: 0, cuDNN Version: 7.5.
W0312 13:33:48.664386  3913 device_context.cc:270] WARNING: device: 0. The installed Paddle is compiled with CUDNN 7.6, but CUDNN version in your machine is 7.5, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
finish initing model from pretrained params from checkpoints/aishell2-C3R71024/step_final
.............
Batch 0 [226/?], current opt (alpha, beta) = (2.500, 5.000),  min [cer] = 0.315554

Final cer:

(alpha, beta) = (0.000, 0.000), [cer] = 0.373080
(alpha, beta) = (0.000, 2.500), [cer] = 0.401285
(alpha, beta) = (0.000, 5.000), [cer] = 0.558224
(alpha, beta) = (0.000, 7.500), [cer] = 1.182910
(alpha, beta) = (0.000, 10.000), [cer] = 2.737783
(alpha, beta) = (2.500, 0.000), [cer] = 0.405473
(alpha, beta) = (2.500, 2.500), [cer] = 0.324490
(alpha, beta) = (2.500, 5.000), [cer] = 0.315554
(alpha, beta) = (2.500, 7.500), [cer] = 0.383133
(alpha, beta) = (2.500, 10.000), [cer] = 0.626082
(alpha, beta) = (5.000, 0.000), [cer] = 0.662106
(alpha, beta) = (5.000, 2.500), [cer] = 0.514661
(alpha, beta) = (5.000, 5.000), [cer] = 0.427255
(alpha, beta) = (5.000, 7.500), [cer] = 0.412734
(alpha, beta) = (5.000, 10.000), [cer] = 0.506004
(alpha, beta) = (7.500, 0.000), [cer] = 0.862050
(alpha, beta) = (7.500, 2.500), [cer] = 0.719073
(alpha, beta) = (7.500, 5.000), [cer] = 0.588104
(alpha, beta) = (7.500, 7.500), [cer] = 0.521363
(alpha, beta) = (7.500, 10.000), [cer] = 0.527786
(alpha, beta) = (10.000, 0.000), [cer] = 0.968724
(alpha, beta) = (10.000, 2.500), [cer] = 0.883552
(alpha, beta) = (10.000, 5.000), [cer] = 0.752025
(alpha, beta) = (10.000, 7.500), [cer] = 0.663502
(alpha, beta) = (10.000, 10.000), [cer] = 0.616029

Finish tuning on 1 batches, final opt (alpha, beta) = (2.500, 5.000)
2020-03-12 15:09:21,420-INFO: finish tuning
Î» dgx-18-04 /DeepSpeech/examples/aishell2
-----------  Configuration Arguments -----------
alpha_from: 2.0
alpha_to: 3.0
batch_size: 256
beam_size: 500
beta_from: 1.0
beta_to: 5.0
cutoff_prob: 1.0
cutoff_top_n: 40
error_rate_type: cer
lang_model_path: models/lm/zhidao_giga.klm
mean_std_path: data/aishell2/mean_std.npz
model_path: checkpoints/aishell2-C3R71024/step_final
num_alphas: 5
num_batches: -1
num_betas: 5
num_conv_layers: 3
num_proc_bsearch: 12
num_rnn_layers: 7
rnn_layer_size: 1024
share_rnn_weights: 0
specgram_type: linear
trainer_count: 8
tune_manifest: data/Wang/16k/manifest.dev
use_gpu: 1
use_gru: 1
vocab_path: data/aishell2/vocab.txt
------------------------------------------------
finish initing model from pretrained params from checkpoints/aishell2-C3R71024/step_final
.............
Batch 0 [226/?], current opt (alpha, beta) = (2.000, 4.000),  min [cer] = 0.299358

Final cer:

(alpha, beta) = (2.000, 1.000), [cer] = 0.329517
(alpha, beta) = (2.000, 2.000), [cer] = 0.310528
(alpha, beta) = (2.000, 3.000), [cer] = 0.303267
(alpha, beta) = (2.000, 4.000), [cer] = 0.299358
(alpha, beta) = (2.000, 5.000), [cer] = 0.306898
(alpha, beta) = (2.250, 1.000), [cer] = 0.346272
(alpha, beta) = (2.250, 2.000), [cer] = 0.322815
(alpha, beta) = (2.250, 3.000), [cer] = 0.313600
(alpha, beta) = (2.250, 4.000), [cer] = 0.308573
(alpha, beta) = (2.250, 5.000), [cer] = 0.315275
(alpha, beta) = (2.500, 1.000), [cer] = 0.359118
(alpha, beta) = (2.500, 2.000), [cer] = 0.331192
(alpha, beta) = (2.500, 3.000), [cer] = 0.317230
(alpha, beta) = (2.500, 4.000), [cer] = 0.310249
(alpha, beta) = (2.500, 5.000), [cer] = 0.315554
(alpha, beta) = (2.750, 1.000), [cer] = 0.380620
(alpha, beta) = (2.750, 2.000), [cer] = 0.346551
(alpha, beta) = (2.750, 3.000), [cer] = 0.328400
(alpha, beta) = (2.750, 4.000), [cer] = 0.319743
(alpha, beta) = (2.750, 5.000), [cer] = 0.321977
(alpha, beta) = (3.000, 1.000), [cer] = 0.401285
(alpha, beta) = (3.000, 2.000), [cer] = 0.366099
(alpha, beta) = (3.000, 3.000), [cer] = 0.340408
(alpha, beta) = (3.000, 4.000), [cer] = 0.327562
(alpha, beta) = (3.000, 5.000), [cer] = 0.330355

Finish tuning on 1 batches, final opt (alpha, beta) = (2.000, 4.000)
-----------  Configuration Arguments -----------
alpha_from: 1.8
alpha_to: 2.2
batch_size: 256
beam_size: 500
beta_from: 3.5
beta_to: 4.5
cutoff_prob: 1.0
cutoff_top_n: 40
error_rate_type: cer
lang_model_path: models/lm/zhidao_giga.klm
mean_std_path: data/aishell2/mean_std.npz
model_path: checkpoints/aishell2-C3R71024/step_final
num_alphas: 5
num_batches: -1
num_betas: 5
num_conv_layers: 3
num_proc_bsearch: 12
num_rnn_layers: 7
rnn_layer_size: 1024
share_rnn_weights: 0
specgram_type: linear
trainer_count: 8
tune_manifest: data/Wang/16k/manifest.dev
use_gpu: 1
use_gru: 1
vocab_path: data/aishell2/vocab.txt
------------------------------------------------
finish initing model from pretrained params from checkpoints/aishell2-C3R71024/step_final
.............
Batch 0 [226/?], current opt (alpha, beta) = (1.800, 4.000),  min [cer] = 0.291818

Final cer:

(alpha, beta) = (1.800, 3.500), [cer] = 0.294610
(alpha, beta) = (1.800, 3.750), [cer] = 0.293493
(alpha, beta) = (1.800, 4.000), [cer] = 0.291818
(alpha, beta) = (1.800, 4.250), [cer] = 0.293493
(alpha, beta) = (1.800, 4.500), [cer] = 0.296286
(alpha, beta) = (1.900, 3.500), [cer] = 0.299637
(alpha, beta) = (1.900, 3.750), [cer] = 0.299358
(alpha, beta) = (1.900, 4.000), [cer] = 0.296844
(alpha, beta) = (1.900, 4.250), [cer] = 0.297961
(alpha, beta) = (1.900, 4.500), [cer] = 0.299637
(alpha, beta) = (2.000, 3.500), [cer] = 0.301871
(alpha, beta) = (2.000, 3.750), [cer] = 0.302988
(alpha, beta) = (2.000, 4.000), [cer] = 0.299358
(alpha, beta) = (2.000, 4.250), [cer] = 0.300195
(alpha, beta) = (2.000, 4.500), [cer] = 0.301312
(alpha, beta) = (2.100, 3.500), [cer] = 0.305501
(alpha, beta) = (2.100, 3.750), [cer] = 0.304384
(alpha, beta) = (2.100, 4.000), [cer] = 0.303267
(alpha, beta) = (2.100, 4.250), [cer] = 0.303826
(alpha, beta) = (2.100, 4.500), [cer] = 0.306618
(alpha, beta) = (2.200, 3.500), [cer] = 0.305222
(alpha, beta) = (2.200, 3.750), [cer] = 0.305501
(alpha, beta) = (2.200, 4.000), [cer] = 0.307456
(alpha, beta) = (2.200, 4.250), [cer] = 0.306618
(alpha, beta) = (2.200, 4.500), [cer] = 0.308294

Finish tuning on 1 batches, final opt (alpha, beta) = (1.800, 4.000)
